# PyTorch 텐서 조작 (2) - 연산 및 Sparse Tensor

## 목차
1. [텐서 연산 및 조작](#1-텐서-연산-및-조작)
2. [Sparse Tensor](#2-sparse-tensor)

---

## 1. 텐서 연산 및 조작

### 1-1. 사칙연산

| 연산 | 함수 | 연산자 | 메서드 |
|------|------|--------|--------|
| 덧셈 | `torch.add(a, b)` | `a + b` | `a.add(b)` |
| 뺄셈 | `torch.sub(a, b)` | `a - b` | `a.sub(b)` |
| 곱셈 (원소별) | `torch.mul(a, b)` | `a * b` | `a.mul(b)` |
| 나눗셈 | `torch.div(a, b)` | `a / b` | `a.div(b)` |

```python
a = torch.tensor([[1, -1], [2, 3]])
b = torch.tensor([[2, -2], [3, 1]])

a + b  # tensor([[3, -3], [5, 4]])
a * b  # tensor([[2, 2], [6, 3]])  ← 원소별 곱셈
```

### 1-2. 통계 함수

> ⚠️ `dim` 파라미터에 따라 결과가 달라짐!

| 함수 | 설명 | dtype 주의 |
|------|------|-----------|
| `torch.sum(t, dim)` | 합계 | - |
| `torch.mean(t, dim)` | 평균 | **float 필수** |
| `torch.max(t, dim)` | 최댓값 | - |
| `torch.min(t, dim)` | 최솟값 | - |
| `torch.argmax(t, dim)` | 최댓값 **인덱스** | - |
| `torch.argmin(t, dim)` | 최솟값 **인덱스** | - |

#### dim 파라미터 이해

```python
t = torch.tensor([[1, 2],
                  [3, 4]])

torch.sum(t)          # tensor(10)  ← 전체 합
torch.sum(t, dim=0)   # tensor([4, 6])  ← 행 방향 (↓) 합
torch.sum(t, dim=1)   # tensor([3, 7])  ← 열 방향 (→) 합
```

```
dim=0: 행 인덱스 변화 방향 (세로 ↓)
dim=1: 열 인덱스 변화 방향 (가로 →)
```

#### mean 사용 시 주의
```python
# ❌ 에러: 정수 텐서
t = torch.tensor([[1, 2], [3, 4]])
torch.mean(t)

# ✅ 정상: float 텐서
t = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)
torch.mean(t)  # tensor(2.5)
```

### 1-3. 벡터/행렬 연산

| 함수 | 설명 | 입력 |
|------|------|------|
| `torch.dot(a, b)` | 벡터 내적 (inner product) | 1D 텐서만 |
| `torch.matmul(A, B)` | 행렬곱 | 2D 이상 |
| `A @ B` | 행렬곱 (연산자) | 2D 이상 |

```python
# 벡터 내적
v1 = torch.tensor([1, 2])
v2 = torch.tensor([3, 4])
torch.dot(v1, v2)  # tensor(11)  ← 1*3 + 2*4

# 행렬곱
A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([[-1, 2], [1, 0]])
torch.matmul(A, B)  # tensor([[1, 2], [1, 6]])
A @ B               # 동일
```

> ⚠️ `mul` (원소별 곱셈) vs `matmul` (행렬곱) 혼동 주의!

---

### 1-4. Broadcasting

Broadcasting은 **크기가 다른 텐서 간 연산**을 가능하게 하는 기능

#### 값 변경에 활용

```python
t = torch.randn(3, 2)

# 특정 행을 스칼라로 변경
t[0, :] = 10  # 0행 전체를 10으로

# 모든 값을 텐서로 변경
t[:, :] = torch.tensor([0, 1])  # 모든 행이 [0, 1]로
```

#### 연산에 활용

```python
A = torch.eye(3)        # (3, 3)
B = torch.tensor([1, 2, 3])  # (3,)

A + B  # B가 (3, 3)으로 확장되어 연산
# tensor([[2., 2., 3.],
#         [1., 3., 3.],
#         [1., 2., 4.]])
```

#### 확장 방향

```python
# 열 벡터 (3,) → 행 방향 확장
B = torch.tensor([1, 2, 3])      # shape: (3,)
# [[1, 2, 3],
#  [1, 2, 3],
#  [1, 2, 3]]

# 행 벡터 (3, 1) → 열 방향 확장  
B = torch.tensor([[1], [2], [3]])  # shape: (3, 1)
# [[1, 1, 1],
#  [2, 2, 2],
#  [3, 3, 3]]
```

#### 차원 불일치 해결

```python
t = torch.randn(3, 2, 5)
mean = t.mean(dim=2)  # shape: (3, 2)

# ❌ 에러: 차원 불일치
t - mean

# ✅ 해결: unsqueeze로 차원 맞추기
t - mean.unsqueeze(-1)  # mean: (3, 2, 1) → broadcasting
```

---

## 2. Sparse Tensor

### 2-1. Sparse Tensor란?

대부분의 원소가 0인 행렬을 효율적으로 저장하는 방식

| 구분 | Dense | Sparse |
|------|-------|--------|
| 저장 방식 | 모든 원소 저장 | 0이 아닌 값만 저장 |
| 메모리 | 크기에 비례 | 0 아닌 값 개수에 비례 |
| 적합한 경우 | 대부분 값이 있을 때 | 대부분 0일 때 |

### 2-2. COO (Coordinate) 형식

0이 아닌 값의 **(행, 열) 좌표**와 **값**을 저장

```python
# Dense → COO Sparse
a = torch.tensor([[0, 2.], [3, 0]])
sparse_a = a.to_sparse()
# indices: [[0, 1], [1, 0]]  ← (행, 열) 좌표
# values: [2., 3.]
# nnz: 2 (non-zero 개수)

# COO Sparse 직접 생성
indices = torch.tensor([[0, 1, 1],   # 행 인덱스
                        [2, 0, 1]])  # 열 인덱스
values = torch.tensor([4, 5, 6])
sparse = torch.sparse_coo_tensor(indices, values, size=(2, 3))

sparse.to_dense()
# tensor([[0, 0, 4],
#         [5, 6, 0]])
```

### 2-3. CSR (Compressed Sparse Row) 형식

**행 기준**으로 압축 저장

```python
# Dense → CSR
t = torch.tensor([[0, 0, 4, 3], 
                  [5, 6, 0, 0]])
sparse_csr = t.to_sparse_csr()
# crow_indices: [0, 2, 4]  ← 각 행의 시작 위치
# col_indices: [2, 3, 0, 1]  ← 열 인덱스
# values: [4, 3, 5, 6]

# CSR 직접 생성
crow = torch.tensor([0, 2, 2])  # 행 포인터
col = torch.tensor([0, 1])      # 열 인덱스
values = torch.tensor([1, 2])
csr = torch.sparse_csr_tensor(crow, col, values)
```

### 2-4. CSC (Compressed Sparse Column) 형식

**열 기준**으로 압축 저장

```python
# Dense → CSC
t = torch.tensor([[0, 0, 4, 3], 
                  [5, 6, 0, 0]])
sparse_csc = t.to_sparse_csc()
# ccol_indices: [0, 1, 2, 3, 4]  ← 각 열의 시작 위치
# row_indices: [1, 1, 0, 0]  ← 행 인덱스
# values: [5, 6, 4, 3]

# CSC 직접 생성
ccol = torch.tensor([0, 2, 2])  # 열 포인터
row = torch.tensor([0, 1])      # 행 인덱스
values = torch.tensor([1, 2])
csc = torch.sparse_csc_tensor(ccol, row, values)
```

### 2-5. 형식 비교

| 형식 | 저장 방식 | 장점 |
|------|----------|------|
| **COO** | (행, 열) 좌표 + 값 | 생성 간편, 직관적 |
| **CSR** | 행 압축 | 행 기준 연산 빠름 |
| **CSC** | 열 압축 | 열 기준 연산 빠름 |

### 2-6. Sparse Tensor 필요성

```python
# 100,000 x 100,000 크기의 Sparse Tensor
i = torch.randint(0, 100000, (2, 100000))
v = torch.rand(100000)
sparse = torch.sparse_coo_tensor(i, v, size=[100000, 100000])
# ✅ 생성 가능 (0이 아닌 값만 저장)

sparse.to_dense()
# ❌ 메모리 아웃! (100억 개 원소 저장 시도)
```

### 2-7. Sparse Tensor 연산

#### 2D Sparse Tensor
```python
sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

# ✅ 모든 연산 가능
torch.add(sparse_a, sparse_b)    # 덧셈
torch.mul(sparse_a, sparse_b)    # 곱셈
torch.matmul(sparse_a, sparse_b) # 행렬곱
```

#### 3D Sparse Tensor
```python
# ✅ 가능
torch.add(sparse_a, sparse_b)  # 덧셈
torch.mul(sparse_a, sparse_b)  # 곱셈 (COO만)

# ❌ 불가능
torch.matmul(sparse_a, sparse_b)  # 행렬곱
```

#### Dense + Sparse 연산
```python
dense_a = torch.tensor([[0, 1], [0, 2]], dtype=torch.float)
sparse_b = torch.tensor([[1, 0], [0, 0]], dtype=torch.float).to_sparse()

# ✅ 가능 (2D)
torch.add(dense_a, sparse_b)
torch.mul(dense_a, sparse_b)
torch.matmul(dense_a, sparse_b)
```

### 2-8. Sparse Tensor 인덱싱

```python
sparse_a = a.to_sparse()

# ✅ 기본 인덱싱 가능
sparse_a[0]  # 0번째 행

# ❌ 슬라이싱 불가능
sparse_a[0, :]  # 에러!
sparse_a[:, 1]  # 에러!
```

---

## 핵심 정리

### 통계 함수 dim 이해

| dim | 방향 | 결과 |
|-----|------|------|
| None | 전체 | 스칼라 |
| 0 | 세로 (↓) | 행 제거 |
| 1 | 가로 (→) | 열 제거 |

### Sparse Tensor 변환

| 변환 | 함수 |
|------|------|
| Dense → COO | `t.to_sparse()` |
| Dense → CSR | `t.to_sparse_csr()` |
| Dense → CSC | `t.to_sparse_csc()` |
| Sparse → Dense | `sparse.to_dense()` |

### Sparse 연산 제한

| 차원 | 덧셈/뺄셈 | 원소곱 | 행렬곱 |
|------|----------|--------|--------|
| 2D | ✅ | ✅ | ✅ |
| 3D | ✅ | ✅ (COO만) | ❌ |

---

## Reference
- [PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)
- [Broadcasting Semantics](https://pytorch.org/docs/stable/notes/broadcasting.html)
- [Sparse Tensor](https://pytorch.org/docs/stable/sparse.html)
