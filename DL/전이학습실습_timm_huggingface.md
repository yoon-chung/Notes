# PyTorch ì „ì´í•™ìŠµ - timmê³¼ Hugging Face í™œìš©

## ëª©ì°¨
1. [ì „ì´í•™ìŠµ ê°œìš”](#ì „ì´í•™ìŠµ-ê°œìš”)
2. [timmì„ í™œìš©í•œ ì „ì´í•™ìŠµ](#1-timmì„-í™œìš©í•œ-ì „ì´í•™ìŠµ)
3. [Hugging Faceë¥¼ í™œìš©í•œ ì „ì´í•™ìŠµ](#2-hugging-faceë¥¼-í™œìš©í•œ-ì „ì´í•™ìŠµ)

---

## ì „ì´í•™ìŠµ ê°œìš”

### ì „ì´í•™ìŠµ(Transfer Learning)ì´ë€?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ì „ì´í•™ìŠµ ê°œë…ë„                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   ì‚¬ì „í•™ìŠµ (Pre-training)          ì „ì´í•™ìŠµ (Fine-tuning)        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚   ëŒ€ê·œëª¨ ë°ì´í„°  â”‚              â”‚   íƒ€ê²Ÿ ë°ì´í„°    â”‚          â”‚
â”‚   â”‚   (ImageNet ë“±) â”‚              â”‚   (CIFAR10 ë“±)  â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚            â”‚                                â”‚                   â”‚
â”‚            â–¼                                â–¼                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  Pretrained     â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚  Fine-tuned     â”‚          â”‚
â”‚   â”‚  Model          â”‚   ê°€ì¤‘ì¹˜ì „ì´  â”‚  Model          â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚   * ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— í™œìš©          â”‚
â”‚   * ì ì€ ë°ì´í„°ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì „ì´í•™ìŠµ ì „ëµ

| ì „ëµ | ì„¤ëª… | ì í•©í•œ ìƒí™© |
|------|------|-------------|
| **Full Fine-tuning** | ì „ì²´ ëª¨ë¸ í•™ìŠµ | ë°ì´í„° ë§ìŒ + ë„ë©”ì¸ ë‹¤ë¦„ |
| **Feature Extraction** | FC Layerë§Œ í•™ìŠµ | ë°ì´í„° ì ìŒ + ë„ë©”ì¸ ìœ ì‚¬ |
| **Gradual Unfreezing** | ì ì§„ì ìœ¼ë¡œ layer í•´ì œ | ì¤‘ê°„ ìƒí™© |

---

## 1. timmì„ í™œìš©í•œ ì „ì´í•™ìŠµ

### 1-1. timm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ

**timm** (PyTorch Image Models): ë‹¤ì–‘í•œ pretrained ì´ë¯¸ì§€ ëª¨ë¸ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
pip install timm
```

### 1-2. ëª¨ë¸ ê²€ìƒ‰ ë° ë¶ˆëŸ¬ì˜¤ê¸°

#### ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ í™•ì¸

```python
import timm

# ì „ì²´ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
timm.list_models()

# íŠ¹ì • ëª¨ë¸ ê²€ìƒ‰ (ì™€ì¼ë“œì¹´ë“œ ì§€ì›)
timm.list_models('resnet*')
# ['resnet18', 'resnet34', 'resnet50', 'resnet101', ...]

# pretrained ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ëª¨ë¸ë§Œ ê²€ìƒ‰
timm.list_models('resnet50', pretrained=True)
# ['resnet50.a1_in1k', 'resnet50.a2_in1k', ...]
```

#### ëª¨ë¸ ìƒì„±

```python
# ê¸°ë³¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ImageNet 1000 í´ë˜ìŠ¤)
model = timm.create_model('resnet50', pretrained=True)

# ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ìˆ˜ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
model = timm.create_model('resnet50', pretrained=True, num_classes=10)
```

#### ëª¨ë¸ ì •ë³´ í™•ì¸

```python
model.default_cfg
# {
#     'input_size': (3, 224, 224),
#     'num_classes': 1000,
#     'mean': (0.485, 0.456, 0.406),
#     'std': (0.229, 0.224, 0.225),
#     ...
# }
```

### 1-3. num_classes ë³€ê²½ ì‹œ ì£¼ì˜ì‚¬í•­

```python
model = timm.create_model('resnet50', pretrained=True, num_classes=10)
```

| Layer | ê°€ì¤‘ì¹˜ ìƒíƒœ |
|-------|------------|
| Conv, BN ë“± (backbone) | **Pretrained ìœ ì§€** âœ… |
| FC Layer (classifier) | **ëœë¤ ì´ˆê¸°í™”** âš ï¸ |

> `num_classes`ë¥¼ ë³€ê²½í•˜ë©´ **FC layerë§Œ ì´ˆê¸°í™”**ë˜ê³ , ë‚˜ë¨¸ì§€ëŠ” pretrained ê°€ì¤‘ì¹˜ ìœ ì§€

---

### 1-4. Fine-tuning ì‹¤ìŠµ (CIFAR10)

#### ì „ì²´ Fine-tuning

```python
model = timm.create_model('resnet50', pretrained=True, num_classes=10).to(device)

lr = 1e-3
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# í•™ìŠµ ë£¨í”„ ì‹¤í–‰
```

#### FC Layerë§Œ Fine-tuning (Feature Extraction)

```python
model = timm.create_model('resnet50', pretrained=True, num_classes=10).to(device)

# ì „ì²´ layer freeze
for param in model.parameters():
    param.requires_grad = False

# FC layerë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
for param in model.fc.parameters():
    param.requires_grad = True

optimizer = optim.Adam(model.parameters(), lr=lr)
```

### 1-5. ì‹¤í—˜ ê²°ê³¼ ë¹„êµ (CIFAR10)

| ì‹¤í—˜ | ì„¤ì • | Test Accuracy |
|------|------|---------------|
| exp1 | Full Fine-tuning (lr=1e-3) | **80.75%** |
| exp2 | FC Layerë§Œ í•™ìŠµ (lr=1e-3) | 44.21% |
| exp3 | Full Fine-tuning (lr=1e-1) | 10.00% âŒ |
| exp4 | Full Fine-tuning (lr=1e-5) | 75.81% |

### 1-6. ì „ì´í•™ìŠµ í•µì‹¬ í¬ì¸íŠ¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì „ì´í•™ìŠµ Learning Rate ì „ëµ                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Learning Rateê°€ ë„ˆë¬´ í¬ë©´ (1e-1):                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚  Pretrained ê°€ì¤‘ì¹˜ê°€ ë§ê°€ì§ (Catastrophic Forgetting)      â”‚
â”‚   â”‚  â†’ í•™ìŠµ ì‹¤íŒ¨ (Accuracy â‰ˆ 10%, ëœë¤ ìˆ˜ì¤€)                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                 â”‚
â”‚   Learning Rateê°€ ì ì ˆí•˜ë©´ (1e-3 ~ 1e-4):                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚  Pretrained ê°€ì¤‘ì¹˜ë¥¼ ì‚´ë¦¬ë©´ì„œ ë¯¸ì„¸ ì¡°ì •                     â”‚
â”‚   â”‚  â†’ ìµœì ì˜ ì„±ëŠ¥                                             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                 â”‚
â”‚   Learning Rateê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ (1e-5):                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚  í•™ìŠµì´ ëŠë¦¼, ìˆ˜ë ´ê¹Œì§€ ë§ì€ epoch í•„ìš”                      â”‚
â”‚   â”‚  â†’ ì„±ëŠ¥ì€ ê´œì°®ì§€ë§Œ ë¹„íš¨ìœ¨ì                                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                 â”‚
â”‚   ğŸ’¡ ê¶Œì¥: ì „ì´í•™ìŠµ ì‹œ lr = 1e-3 ~ 1e-4 (ì¼ë°˜ í•™ìŠµë³´ë‹¤ ì‘ê²Œ)    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Hugging Faceë¥¼ í™œìš©í•œ ì „ì´í•™ìŠµ

### 2-1. Hugging Face ì†Œê°œ

**Hugging Face**: NLP ì¤‘ì‹¬ì˜ pretrained ëª¨ë¸ í—ˆë¸Œ (transformers ë¼ì´ë¸ŒëŸ¬ë¦¬)

```bash
pip install transformers
```

### 2-2. BERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°

```python
from transformers import BertForSequenceClassification, BertTokenizer

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = BertForSequenceClassification.from_pretrained("bert-base-cased")

# í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

### 2-3. BERT ì…ë ¥ í˜•ì‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BERT ì…ë ¥ í˜•ì‹                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   ì›ë³¸ ë¬¸ì¥:   "I love this movie"                              â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â–¼                                          â”‚
â”‚   BERT í˜•ì‹:  [CLS] I love this movie [SEP]                     â”‚
â”‚                 â”‚                        â”‚                      â”‚
â”‚                 â”‚                        â””â”€ ë¬¸ì¥ ë í† í°         â”‚
â”‚                 â””â”€ ë¶„ë¥˜ìš© í† í° (Classification)                  â”‚
â”‚                                                                 â”‚
â”‚   Tokenize:   ['[CLS]', 'I', 'love', 'this', 'movie', '[SEP]']  â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â–¼                                          â”‚
â”‚   Token IDs:  [101, 146, 1567, 1142, 2523, 102]                 â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2-4. ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

#### Step 1: [CLS], [SEP] í† í° ì¶”ê°€

```python
# ë¬¸ì¥ ì•ë’¤ì— íŠ¹ìˆ˜ í† í° ì¶”ê°€
train['review'] = train['review'].apply(lambda x: f'[CLS] {x} [SEP]')
```

#### Step 2: Tokenization

```python
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

# í† í°í™”
tokenized_texts = list(map(lambda x: tokenizer.tokenize(x), sentences))

# í† í° â†’ ID ë³€í™˜
input_ids = list(map(lambda x: tokenizer.convert_tokens_to_ids(x), tokenized_texts))
```

#### Step 3: Padding

```python
MAX_LEN = 128

def zero_padding(id_list, max_len):
    return np.array([
        i[:max_len] if len(i) >= max_len 
        else i + [0] * (max_len - len(i)) 
        for i in id_list
    ])

input_ids = zero_padding(input_ids, MAX_LEN)
```

#### Step 4: Attention Mask ìƒì„±

```python
# íŒ¨ë”©(0)ì´ ì•„ë‹Œ ë¶€ë¶„ë§Œ True (attention ê³„ì‚° ëŒ€ìƒ)
attention_masks = input_ids > 0
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Attention Mask ì—­í•                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Input IDs:      [101, 146, 1567, 102, 0, 0, 0, 0]            â”‚
â”‚                     â”‚    â”‚    â”‚    â”‚   â”‚  â”‚  â”‚  â”‚              â”‚
â”‚   Attention Mask: [ 1,   1,   1,   1,  0, 0, 0, 0]             â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚
â”‚                          ì‹¤ì œ í† í°        íŒ¨ë”© (ë¬´ì‹œ)            â”‚
â”‚                        (attention ê³„ì‚°)                         â”‚
â”‚                                                                 â”‚
â”‚   * íŒ¨ë”© ë¶€ë¶„ì€ attention ê³„ì‚°ì—ì„œ ì œì™¸ â†’ ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2-5. Custom Dataset êµ¬ì„±

```python
class EmotionData(torch.utils.data.Dataset):
    def __init__(self, inputs, masks, labels):
        self.inputs = inputs
        self.masks = masks
        self.labels = labels

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        return self.inputs[idx], self.masks[idx], self.labels[idx]
```

### 2-6. BERT Fine-tuning

```python
model = BertForSequenceClassification.from_pretrained("bert-base-cased").to(device)

optimizer = optim.Adam(model.parameters(), lr=1e-5)

for epoch in range(num_epochs):
    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)
        
        # Forward
        output = model(
            input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        loss = output['loss']      # CrossEntropy Loss
        logits = output['logits']  # ì˜ˆì¸¡ í™•ë¥ 
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 2-7. ì‹¤í—˜ ê²°ê³¼ ë¹„êµ (IMDB ê°ì„±ë¶„ë¥˜)

| ì‹¤í—˜ | ì„¤ì • | Test Accuracy |
|------|------|---------------|
| bert1 | Full Fine-tuning (lr=1e-5) | **88.14%** |
| bert2 | FC Layerë§Œ í•™ìŠµ (lr=1e-5) | 59.68% |
| bert3 | Full Fine-tuning (lr=1e-4) | 50.76% âŒ |

---

## í•µì‹¬ ì •ë¦¬

### timm vs Hugging Face ë¹„êµ

| êµ¬ë¶„ | timm | Hugging Face |
|------|------|--------------|
| **ì£¼ìš” ë¶„ì•¼** | Computer Vision | NLP (+ Vision) |
| **ëŒ€í‘œ ëª¨ë¸** | ResNet, EfficientNet, ViT | BERT, GPT, T5 |
| **ì„¤ì¹˜** | `pip install timm` | `pip install transformers` |
| **ëª¨ë¸ ë¡œë“œ** | `timm.create_model()` | `Model.from_pretrained()` |

### ì „ì´í•™ìŠµ Best Practice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì „ì´í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Learning Rate ì„¤ì •                                          â”‚
â”‚     âœ… ì¼ë°˜ í•™ìŠµë³´ë‹¤ ì‘ê²Œ (1e-3 ~ 1e-5)                         â”‚
â”‚     âŒ ë„ˆë¬´ í¬ë©´ pretrained ê°€ì¤‘ì¹˜ ì†ìƒ                          â”‚
â”‚                                                                 â”‚
â”‚  2. Fine-tuning ì „ëµ ì„ íƒ                                       â”‚
â”‚     â€¢ ë°ì´í„° ë§ìŒ + ë„ë©”ì¸ ë‹¤ë¦„ â†’ Full Fine-tuning              â”‚
â”‚     â€¢ ë°ì´í„° ì ìŒ + ë„ë©”ì¸ ìœ ì‚¬ â†’ FC Layerë§Œ í•™ìŠµ               â”‚
â”‚                                                                 â”‚
â”‚  3. ì…ë ¥ ì „ì²˜ë¦¬                                                 â”‚
â”‚     â€¢ timm: ëª¨ë¸ë³„ input_size, mean, std í™•ì¸                   â”‚
â”‚     â€¢ BERT: [CLS], [SEP] í† í° + Attention Mask                  â”‚
â”‚                                                                 â”‚
â”‚  4. Pretrained Tokenizer ì‚¬ìš© (NLP)                             â”‚
â”‚     âœ… ë°˜ë“œì‹œ ëª¨ë¸ê³¼ ê°™ì€ tokenizer ì‚¬ìš©                        â”‚
â”‚     âŒ ë‹¤ë¥¸ tokenizer ì‚¬ìš© ì‹œ ë‹¨ì–´-ID ë§¤í•‘ ë¶ˆì¼ì¹˜               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer Freeze ì½”ë“œ íŒ¨í„´

```python
# ì „ì²´ freeze
for param in model.parameters():
    param.requires_grad = False

# íŠ¹ì • layerë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ
for param in model.fc.parameters():        # timm (ResNet)
    param.requires_grad = True

for param in model.classifier.parameters(): # Hugging Face (BERT)
    param.requires_grad = True
```

---

## Reference
- [timm ê³µì‹ ë¬¸ì„œ](https://timm.fast.ai/)
- [timm GitHub](https://github.com/huggingface/pytorch-image-models)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
- [BERT ë…¼ë¬¸](https://arxiv.org/abs/1810.04805)
- [PyTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/index.html)
