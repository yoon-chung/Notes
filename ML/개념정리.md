# 개념정리

## 1. 인공지능 ⊃ 머신러닝 ⊃ 딥러닝
- 머신러닝: 함수공간에서 손실함수를 최적화하는 파라미터를 찾는 방식으로 구현
- 딥러닝: 심층신경망 구조의 모델을 사용하는 머신러닝의 일종

## 2. Loss와 학습
- Loss: 추측값 y hat과 목표값 y의 차이의 절대값
- 학습: 손실함수를 최소화하는 파라미터를 찾는것

## 3. 정형 vs 비정형
- 정형 데이터: 정량적 정보, 구조화 (테이블형태, 엑셀, csv)
- 비정형 데이터: 정성적 정보, 비구조화 (텍스트, 이미지, 음성)

## 4. NLP, CV, RecSystem
- NLP: 사람이 쓰는 언어, 문장, 단어를 ML 통해서 숫자로 표현해서 모델로 학습
- CV: 이미지, 동영상 데이터 다루는 ML 연구분야
- 비트맵: RGB의 강도를 0~255사이 정수로 나타내고 색상을 길이 3의 벡터로 표현하고 이미지 해상도(가로길이 x 세로길이)만큼 쌓아 이미지를 표현하는 방식
- 추천시스템: 개인관심사에 맞는 컨텐츠 제공. (유저의 피드백 정보, 구매기록, 웹페이지 방문기록)

## 5. EDA (선택적 데이터 탐색)
- Bias(편향), 라벨 노이즈 (결측,이상,중복), 모호한 데이터(명확한 기준 또는 폐기 필요)를 처리
- 기본정보 파악 -> 이상치,결측치 파악 -> Boxplot -> 변수별 통계량과 분포 확인 -> Correlation plot -> 데이터셋 전처리(이상치 결측치 처리) -> 피쳐 scale 조정(표준화, 정규화)
- 상관계수: 
  - 공선성(Colinearity): 한 변수가 다른 변수와 사실상 같은 값으로 표현
  - 다중선성(Multicolinearity): 한 변수가 평균값 등 다른 여러 변수의 조합으로 나타날때
    예) “수익”이라는 변수가 각각 달러, 원화라는 또다른 변수로 표현될때


## 6. ML 방법론

### 6-1. 지도학습
**정답을 직접 지정, 즉 사람이 라벨(y)를 설정, 함수의 출력값이 라벨(y)에 가까워지게 학습하는 방식**
1. 회귀모델: 종속변수 y가 연속형
  - 선형회귀모델, 상관관계 분석
  - 로지스틱 회귀모델
2. 분류모델: 종속변수 y가 범주형/이산형 
  - KNN, Decision Tree, SVM

### 6.2. 비지도학습
**라벨 y_train을 활용하지 않고 입력데이터 x로만 모델을 학습하는 방식**

0. 매니폴드 가설: 일반적인 고차원 데이터들은 가능한 모든 데이터 공간에 완전히 고르게 분포하는게 아닌, 상대적으로 매우 낮은 차원으로 이해할 수 있는 매니폴드(곡면과 유사한 개념)을 이루고 있다. -> 이에 따라 비지도학습의 여러 연구분야가 가능

0-1. 고차원 데이터를 다룰때 생기는 문제점들
- 컴퓨터 자원의 사용량
- 데이터 Sparsity(희소성): 데이터 간 거리가 지수적으로 증가하여 공간내 밀도가 낮아져 모델 학습이 어려워짐
- 직관적으로 이해하기 힘든 고차원 공간

1. 차원축소 : 고차원 데이터 정보를 최대한 보존하면서 훨씬 적은 차원으로 표현할 수 있는 방법을 찾는것
  - PCA(주성분분석)
  - t-SNE: 고차원 데이터를 차원축소한 구조를 알기 쉽게 보여주는 시각화 방법
  - 노이즈 제거: 차원축소를 하면 정보가 유실되는 사실을 역으로 활용해, 원본 데이터의 노이즈를 제거
2. 군집화(Clustering): k-Means, DBSCAN
  - 데이터 패턴(**유사성**)을 파악해 여러개 cluster로 나누는것
  - **지도학습의 "분류"** : 모델의 결과와 레이블(y)을 비교해 점점 레이블에 가까운 출력을 내놓도록 학습
  - **비지도학습의 "군집"** : 가장 유사한 샘플끼리 묶이게 될지 데이터간 **관계**를 비교해 학습
  - 이상값 탐지(Anomaly Detection): 매니폴드 구조를 학습하고 거기에 벗어나있는 datapoint찾기
3. 임베딩 공간과 생성모델
  - 임베딩: NLP의 방법론. 자연어 단어들을 의미적 연산이 가능한 임베딩 공간에 매핑
  - 생성모델(generative model): 저차원 임베딩 공간에서 원래의 이미지공간으로 가는 함수를 학습해 기존에 없던, 새로운 feature를 가진 샘플을 생성해냄

## 7. 선형회귀
- 단순선형회귀모델
  - MSE 손실함수: 목표값과 출력값 간 차이의 제곱

- 상관관계 분석
  - 상관관계: 한 변수가 변화할 때 다른 변수도 함께 변화하는 경향성 보임
  - 인과관계: 한 변수의 변화가 원인이 되어 그 결과로 다른 변수를 변화시킬 때.
  - 4가지 가정 
    - 선형성: 두 변인 X와 Y의 관계가 직선적이어야
    - 등분산성: X의 값에 관계없이 Y의 분산이 일정해야
    - 정규성: 각 변인은 모두 정규분포를 따라야
    - 독립성: 각 샘플들은 모두 독립적이어야

- 상관계수: 상관관계의 강도
  - 공선성(Colinearity): 한 변수가 다른 변수와 사실상 같은 값으로 표현. (상관계수 1 or -1)
  - 다중공선성(Multicolinearity): 한 변수가 평균값 등 다른 여러 변수의 조합으로 나타날때. 
  - 예) “수익”이라는 변수가 각각 달러, 원화라는 또다른 단위로 표기되어 중복될 때


## 8. 분류

### 8-1. 선형모델 활용
1. 이진분류 : 결과값이 1일 확률을 예측
- 로지스틱 함수 : 인풋을 0, 1 사이로 변환해 확률로 해석될 수 있는 결과를 내는 함수
- 시그모이드 함수 (= 로지스틱 함수 in 딥러닝)
- 크로스엔트로피 (Binary CE) : 로지스틱 함수를 통과한, 모델의 출력값 확률에 대해 적용하도록 만들어진 손실함수. 
- 회귀모델에서 쓰던 MSE 대신, BCE를 사용하면 학습이 더 잘되는 이유:
  - 모델의 예측 결과가 완전히 틀린 경우, BCE는 매우 큰 패널티를 주지만 MSE는 그렇지 않음
  - 모델이 정답에 어느정도 근접한 결과를 내놓은 경우 MSE는 로스값이 너무 작아져 학습이 매우 느려짐
  - 로지스틱함수와 결합했을 때 미분식이 매우 간단해진다

2. 다중분류 : 결과값들의 합이 1 : [0.8, 0.2, 0.0]
- 원핫인코딩
- 소프트맥스: 결과 logit 벡터를 각각 클래스에 대한 확률을 나타내는 벡터로 변환해주는 함수
- 크로스엔트로피 (CE)

### 8-2. 기타방법
1. kNN (k-nearest neighbor) : 가장 근접한 k개 라벨을 기준으로 출력값을 결정
2. Decision tree : x 내의 대소 관계나 특정 임계값(threshold)와의 비교 등 판단을 계층적으로 적용
  - 불순도를 최소화 (손실함수와 같은 역할)하는 방향으로 기준값을 정함
  - 랜덤 포레스트 : 각 트리의 결과를 종합해 최종 출력값을 결정하는 방식. 앙상블
3. SVM : 두 클래스를 가장 잘 분리하는 결정 경계를 찾아내는 방법론. Margin값 (데이터포인트들~ 결정경게 사이 거리)을 최대화하도록 학습.
  - 커널 트릭 : 데이터를 고차원으로 변환해 선형분리가 가능하도록 만드는 기법 

### 9. 차원축소와 시각화
1. SVD(Singular Vector Decomposition) 행렬분해
임의의 m x n 행렬을, 세 개의 행렬의 곱으로 분해하는 선형대수학 기법으로, 고차원 데이터를 효율적으로 압축하고 핵심 정보를 추출하는 데 널리 사용
- $$A = U\Sigma V^T$$: 임의의 m x n 행렬 A
- $U$ (Left Singular Vectors): m x m 직교행렬(Orthogonal Matrix)로, 데이터의 새로운 축을 정의
- $\Sigma$ (Singular Values): 대각행렬(Diagonal Matrix)로, 각 주성분의 중요도(데이터의 분산 정도)를 나타내는 특이값을 담고 있음. 행렬의 eigen value와 관련. 
- $V^T$ (Right Singular Vectors): n x n 직교행렬의 전치로, 원래 변수들과 새로운 축 간의 관계를 나타냄
- rank가 r인(r개의 singular value를 가지는) 행렬 A가 있을 때, SVD를 활용해 이를 r개의 rank1 matrix들의 합으로 표현 가능

2. Truncated SVD를 통한 행렬의 rank 축소
r개의 성분들 중 크기가 작은 것들을 버리고 행렬 M을 낮은 rank의 행렬로 나타내는 차원축소 방법론

3. PCA: 주성분 분석을 통한 분산 보존

4. t-SNE:  데이터 간 거리 확률을 이용한 비선형 시각화

### 10. 군집화
1. k-means clustering
- k개의 군집을 나눌 때 평균(mean)을 이용
- 거리함수  L2 norm: 각각의 데이터 포인트로부터 가장 가까운 군집의 중심까지 거리의 제곱의 가중합이 최소가 되도록
- 각 데이터 포인트가 특정 군집에 할당되면 해당 군집의 중심과의 거리가 다른 어떤 군집중심과의 거리보다 작아야
- __elbow method__: 데이터에 대한 사전 지식이 거의 없을 때, 데이터를 몇 개의 군집으로 나누어야 할 지 모를 때 사용. 군집의 수에 대해서 모두 군집화를 수행한 후 적절하게 L2 norm이 줄어든 시점을 정하는 방법 (L2 norm: 각 데이터와 할당된 군집의 중심까지의 거리의 제곱의 가중합) 
2. k-medoids: medoids(중앙값)으로 중심 설정하여 k-means보다 더 robust한 성능

### 11. 추천시스템
1. 구성요소와 활용분야
- 활동기록: 사용자의 모든 행동
- 피드백: 선호 or 비선호
- 유저 프로필: 유저에 대한 정보를 모아놓은 데이터
- 아이템 프로필: 예) 각 콘텐츠에 대한 정보들(영화장르, 감독,배우..)
- 스트리밍서비스, 소셜네트워크서비스, 교육서비스, 금융서비스, 광고플랫폼

2. 기초방법론
1) 인기도기반
- 레딧공식: 사용자의 추천/비추천, 게시물 올라온 시간 고려해 인기도 측정
- 구글의 페이지 랭크 방식: 웹페이지 간 상호 연결관계를 활용
2) 연관규칙 (Association Rule Analysis)
- DB내에서 동시에 등장하는 빈도가 높은 컨텐츠의 조합을 찾아내는 방법론
3) 콘텐츠 기반 추천(CBF)
4) 협업필터링: 사용자들 또는 아이템 사이 유사성 기반
- 코사인 유사도: 두 벡터 사이 각도의 코사인 값 기준 (-1~1사이, 서로 유사할수록 1에 가까움)
4-1) 기억 기반(Memory-based): 
- 사용자-아이템 선호도 행렬: 모든 사용자-아이템 쌍에 대해 선호도를 수치로 나타낸 행렬.
  대부분 유저는 전체 DB 아이템 중 매우 적은 일부만 상호작용하므로, 행렬은 대부분 값이 0인 희소행렬(Sparse Matrix)
- 사용자 기반: 예) 유저B의 선호도 패턴은 유저A와 비슷하므로, A가 긍정평가한 아이템1을 추천한다.
- 아이템 기반: 예) 아이템2를 좋아하는 유저는 아이템1 좋아하는 경향 있으므로 유저A에게 추천
- 잠재요인 분석: 비지도학습의 임베딩과 같은 개념
4-2) 한계점
- Cold Start문제: 선호도 데이터 없는 신규 유저의 경우
- 개인정보 문제: 법률적
- 시간에 따른 선호도 변화 문제

